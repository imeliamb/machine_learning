{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43235cc3-8d42-4a37-b55f-21d9f56c8adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T22:07:40.356998Z",
     "iopub.status.busy": "2023-06-11T22:07:40.356266Z",
     "iopub.status.idle": "2023-06-11T22:07:46.828132Z",
     "shell.execute_reply": "2023-06-11T22:07:46.827063Z",
     "shell.execute_reply.started": "2023-06-11T22:07:40.356974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 18:07:41.003424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"/SNS/users/m2d/git/machine_learning/src\")\n",
    "import importlib\n",
    "import models\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd188d7-6770-4190-9f56-07f018b22874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:26:33.126170Z",
     "iopub.status.busy": "2023-06-11T16:26:33.125799Z",
     "iopub.status.idle": "2023-06-11T16:26:33.788879Z",
     "shell.execute_reply": "2023-06-11T16:26:33.788280Z",
     "shell.execute_reply.started": "2023-06-11T16:26:33.126153Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000,)\n",
      "(1000000, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.expanduser(\"~m2d/git/analysis_playground/ml/data/training_data\")\n",
    "\n",
    "pars = np.load(os.path.join(data_dir, \"orhs3-noise_pars.npy\"))[:1000000]\n",
    "refl = np.load(os.path.join(data_dir, \"orhs3-noise_data.npy\"))[:1000000]\n",
    "q_values = np.load(os.path.join(data_dir, \"orhs3_q_values.npy\"))\n",
    "\n",
    "print(pars.shape)\n",
    "print(refl.shape)\n",
    "print(q_values.shape)\n",
    "\n",
    "idx = 0\n",
    "nset=10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa46ee0a-cbf9-49f8-b395-80975f8ea7e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T15:36:54.044502Z",
     "iopub.status.busy": "2023-06-11T15:36:54.044173Z",
     "iopub.status.idle": "2023-06-11T15:36:54.048438Z",
     "shell.execute_reply": "2023-06-11T15:36:54.048070Z",
     "shell.execute_reply.started": "2023-06-11T15:36:54.044483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. ... 2. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943a02fb-6aa7-4ed7-ac92-9740f71b5417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:26:38.819790Z",
     "iopub.status.busy": "2023-06-11T16:26:38.819439Z",
     "iopub.status.idle": "2023-06-11T16:26:40.467621Z",
     "shell.execute_reply": "2023-06-11T16:26:40.466456Z",
     "shell.execute_reply.started": "2023-06-11T16:26:38.819770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onelayerpars= []\n",
    "onelayerrefl= []\n",
    "twolayerpars= []\n",
    "twolayerrefl= []\n",
    "threelayerpars= []\n",
    "threelayerrefl= []\n",
    "\n",
    "for i in range ( len(pars)-1 ):\n",
    "    if pars [i]==1:\n",
    "        onelayerpars.append(pars[i])\n",
    "        onelayerrefl.append(refl[i])\n",
    "    if pars [i]==2:\n",
    "        twolayerpars.append(pars[i])\n",
    "        twolayerrefl.append(refl[i])\n",
    "    else:\n",
    "        threelayerpars.append(pars[i])\n",
    "        threelayerrefl.append(refl[i])\n",
    "                       \n",
    "onepars=np.asarray(onelayerpars)\n",
    "onerefl=np.asarray(onelayerrefl)\n",
    "twopars=np.asarray(twolayerpars)\n",
    "tworefl=np.asarray(twolayerrefl)\n",
    "threepars=np.asarray(threelayerpars)\n",
    "threerefl=np.asarray(threelayerrefl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b923b9-9ef4-4a67-83ab-368faee9e378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:26:42.982215Z",
     "iopub.status.busy": "2023-06-11T16:26:42.981864Z",
     "iopub.status.idle": "2023-06-11T16:26:42.985892Z",
     "shell.execute_reply": "2023-06-11T16:26:42.985376Z",
     "shell.execute_reply.started": "2023-06-11T16:26:42.982194Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "refl= np.asarray(threerefl[:-nset])\n",
    "pars= np.asarray (threepars[:-nset])\n",
    "\n",
    "testset= np.asarray(threerefl[-nset:])\n",
    "testsetout= np.asarray(threepars[-nset:])\n",
    "#testpars = np.asarray (threepars[-nset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f6dbfb-1df1-4c09-9c44-a6216372127b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:26:50.687688Z",
     "iopub.status.busy": "2023-06-11T16:26:50.687355Z",
     "iopub.status.idle": "2023-06-11T16:32:08.165814Z",
     "shell.execute_reply": "2023-06-11T16:32:08.164965Z",
     "shell.execute_reply.started": "2023-06-11T16:26:50.687663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 12:26:50.798331: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2565/2565 [==============================] - ETA: 0s - loss: 0.3748"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 1470, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 9 and 100 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential/dense_1/BiasAdd, IteratorGetNext:1)' with input shapes: [?,9], [?,100].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m encoder\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mcnn_encoder(n_data, n_features, n_outputs)\n\u001b[1;32m      8\u001b[0m encoder\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m , optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam())\n\u001b[0;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedfg736df.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda/envs/tensorflow/lib/python3.11/site-packages/keras/losses.py\", line 1470, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 9 and 100 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential/dense_1/BiasAdd, IteratorGetNext:1)' with input shapes: [?,9], [?,100].\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(models)\n",
    "n_data = refl.shape[0]\n",
    "n_features = refl.shape[1]\n",
    "n_outputs = 9\n",
    "\n",
    "encoder=models.cnn_encoder(n_data, n_features, n_outputs)\n",
    "encoder.compile(loss='mean_squared_error' , optimizer=tf.keras.optimizers.Adam())\n",
    "history = encoder.fit(refl, pars,\n",
    "                        epochs=20, batch_size=256,\n",
    "                        validation_data=(testset, testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb22a06-8f3a-4e1a-ad42-dafcceb98f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100, figsize=[8,4])\n",
    "\n",
    "plt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.xscale('linear')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
